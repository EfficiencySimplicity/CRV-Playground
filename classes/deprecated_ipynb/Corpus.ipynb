{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f888947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Utils.ipynb\n",
    "%run CRV.ipynb\n",
    "%run Vectorizer.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc2f449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from spellwise import CaverphoneOne\n",
    "from itertools import *\n",
    "import numpy as np\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e40942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################ The Corpus Class ################\n",
    "#\n",
    "# -Stores a list of sentences and information about them\n",
    "# -Loads and cleans data\n",
    "# -Optionally lemmatizes (cleanliness -> clean ly ness) and corrects spelling errors\n",
    "# -Splits each sentence into a list of words\n",
    "# -Collects information\n",
    "# -Creates CRVs\n",
    "# -Creates a vectorizer\n",
    "\n",
    "\n",
    "class Corpus:\n",
    "    def __init__(self,\n",
    "                 filepath,               # no need for '.json'\n",
    "                 file_indexer  = None,   # some jsons have metadata, and so you need to index the actual text\n",
    "                 text_mode     = 'word', # 'word' or 'char'\n",
    "                 spell_correct = False,  # spell correct (fails on niche words)\n",
    "                 lemmatize     = True,   # remove 'er', 'ed', 's', 'ing', etc. to the best of its ability\n",
    "                 removal_threshold = 0,  # replace a word with <= this many appearences in the corpus with <UNK>\n",
    "                 log = True):\n",
    "\n",
    "        # setup\n",
    "        self.text_mode = text_mode\n",
    "        self.load_sentences(filepath, file_indexer)\n",
    "\n",
    "        # clean data\n",
    "        if spell_correct != False:\n",
    "            self.spell_correcter = CaverphoneOne()\n",
    "            self.spell_correcter.add_from_path(spell_correct)\n",
    "            self.spell_correct()\n",
    "\n",
    "        self.lemmatize() if lemmatize else None\n",
    "        self.remove_uncommon(removal_threshold) if removal_threshold >= 1 else None\n",
    "        \n",
    "        # post_cleanup regathering\n",
    "        self.scrape_data()\n",
    "\n",
    "        # log info\n",
    "        if not log:\n",
    "            return\n",
    "\n",
    "        print(\"Corpus loaded:\")\n",
    "        print(\"    - \" + str(len(self.sentences)) + \" sentences.\")\n",
    "        print(\"    - longest sentence: \\n\")\n",
    "        print(concat_sentence(max(self.sentences, key = len), self.text_mode))\n",
    "        print(\".    \")\n",
    "        print(\"    - shortest sentence: \\n\")\n",
    "        print(concat_sentence(min(self.sentences, key = len), self.text_mode))\n",
    "\n",
    "\n",
    "    # Data loading and standardization\n",
    "\n",
    "    def load_sentences(self, filepath, file_indexer = None):\n",
    "        if filepath[-5:] != '.json':\n",
    "            filepath += '.json'\n",
    "\n",
    "        with open(filepath) as f:\n",
    "            if file_indexer is None:\n",
    "                self.sentences = json.load(f)\n",
    "            else:\n",
    "                self.sentences = [item[file_indexer] for item in json.load(f)]\n",
    "\n",
    "        # all lowercase, remove duplicates, split all\n",
    "        self.sentences = [sentence.lower() for sentence in self.sentences]\n",
    "        self.sentences = list(set(self.sentences))\n",
    "        self.sentences = [split_sentence(sentence, self.text_mode) for sentence in self.sentences]\n",
    "\n",
    "    # data collection\n",
    "\n",
    "    def get_word_counts_and_vocab(self):\n",
    "        self.word_counts = sort_hl(dict(Counter(chain(*self.sentences))))\n",
    "        self.vocab = list(self.word_counts.keys())\n",
    "        self.set_vocab = set(self.vocab)\n",
    "        \n",
    "\n",
    "    def scrape_data(self, log = True):\n",
    "\n",
    "        self.get_word_counts_and_vocab()\n",
    "\n",
    "        self.sentence_indices = defaultdict(set)\n",
    "\n",
    "        for i, sentence in enumerate(self.sentences):\n",
    "            for word in set(sentence):\n",
    "                self.sentence_indices[word].add(i)\n",
    "\n",
    "        self.word_indices = {word : i for i, word in enumerate(self.vocab)}\n",
    "        self.total_word_count = sum(self.word_counts.values())\n",
    "        self.total_unique_word_count = len(self.vocab)\n",
    "        self.word_percentages = {item[0] : item[1] / self.total_word_count for item in self.word_counts.items()}\n",
    "        self.max_length = len(max(self.sentences, key = len))\n",
    "\n",
    "        # log info\n",
    "        if not log:\n",
    "            return\n",
    "    \n",
    "        print(\"Data collected:\")\n",
    "        print(\"    - \" + str(self.total_unique_word_count) + \" unique words found.\")\n",
    "        print(\"    - most common words: \" + str(list(self.word_counts.keys())[:5]))\n",
    "        print(\"    - least common words: \" + str(list(self.word_counts.keys())[-5:]))\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "    # Data cleanup\n",
    "\n",
    "    def spell_correct(self):\n",
    "        self.get_word_counts_and_vocab()\n",
    "        self.replace(self.get_correctable_words())\n",
    "\n",
    "    def lemmatize(self):\n",
    "        self.get_word_counts_and_vocab()\n",
    "        self.replace(self.get_lemmatizable_words())\n",
    "\n",
    "    def remove_uncommon(self, n = 0):\n",
    "        self.get_word_counts_and_vocab()\n",
    "        self.replace({word : ['<UNK>'] for word in self.set_vocab if self.word_counts[word] <= n})        \n",
    "\n",
    "    def get_correctable_words(self, word_set = None):\n",
    "        possible_corrections = defaultdict(str)\n",
    "\n",
    "        for word in word_set or (self.set_vocab - set(['<START>', '<END>', '<UNK>'])):\n",
    "            result = self.get_correction(word)\n",
    "            if result != word:\n",
    "                possible_corrections[word] = result\n",
    "\n",
    "        return possible_corrections\n",
    "    \n",
    "    def get_correction(self, word):\n",
    "        if len(word) <= 3:\n",
    "            return word\n",
    "\n",
    "        corrections = self.spell_correcter.get_suggestions(word)\n",
    "        corrections = {item['word'] : item['distance'] for item in corrections}\n",
    "        same_keys = set(corrections.keys()).intersection(self.set_vocab)\n",
    "\n",
    "        if len(same_keys) == 0:\n",
    "            return word\n",
    "\n",
    "        if word in same_keys:\n",
    "            return word\n",
    "\n",
    "        corrections = {key : self.word_counts[key] for key in same_keys}\n",
    "        return list(sort_hl(corrections).keys())[0:1]\n",
    "    \n",
    "\n",
    "    def spelled_correct(self, word):\n",
    "        return self.get_correction(word) == word\n",
    "\n",
    "\n",
    "\n",
    "    def get_lemmatizable_words(self, word_set = None):\n",
    "        lemmatizable_words = defaultdict(str)\n",
    "\n",
    "        for word in word_set or self.vocab:\n",
    "            if len(word) <= 4:\n",
    "                continue\n",
    "\n",
    "            # word ends in 's'\n",
    "            if (word[-1] == 's') and (word[-2:] != 'ss') and (word[-3:] != 'ous'):\n",
    "                # some words with a y change spelling (candy -> candies)\n",
    "                if word[-3:] == 'ies' and word[:-3] + 'y' in self.vocab:\n",
    "                    lemmatizable_words[word] = lemmatize(word, '_ies->_y+s')\n",
    "                # words that normally end in 's' have 'es' added\n",
    "                elif word[-3:] == 'ses' and word[:-2] in self.vocab:      \n",
    "                    lemmatizable_words[word] = lemmatize(word, '_es->_+s')\n",
    "                # words uses usual 's' rules\n",
    "                elif word[:-1] in self.vocab:\n",
    "                    lemmatizable_words[word] = lemmatize(word, '_s->_+s')\n",
    "\n",
    "            # word ends in 'ly'\n",
    "            elif word[-2:] == 'ly':\n",
    "                if word[:-1] + 'e' in self.set_vocab:\n",
    "                    lemmatizable_words[word] = lemmatize(word, '_y->_e+ly')# fully\n",
    "                elif word[:-2] in self.set_vocab:\n",
    "                    lemmatizable_words[word] = lemmatize(word, '_ly->_+ly')# fully\n",
    "\n",
    "            # word ends in 'ness'\n",
    "            elif word[-4:] == 'ness':\n",
    "                lemmatizable_words[word] = lemmatize(word, '_ness->_+ness')\n",
    "\n",
    "            elif word[-4:] == 'less' and word != 'less':\n",
    "                lemmatizable_words[word] = lemmatize(word, '_less->_+less')\n",
    "\n",
    "            # word ends in 'ing'\n",
    "            elif word [-3:] == 'ing':\n",
    "                # putting -> put ing\n",
    "                if word[-5] == word[-4] and word[:-4] in self.vocab:\n",
    "                    lemmatizable_words[word] = lemmatize(word, '_?ing->_+ing')\n",
    "                # baking -> bake ing\n",
    "                elif word[:-3] + 'e' in self.vocab:\n",
    "                    lemmatizable_words[word] = lemmatize(word, '_ing->_e+ing')\n",
    "                # working -> work ing\n",
    "                elif word[:-3] in self.vocab and len(word) > 5:\n",
    "                    lemmatizable_words[word] = lemmatize(word, '_ing->_+ing')\n",
    "\n",
    "            # word ends in 'er'\n",
    "            elif (word != 'er') and (word[-2:] == 'er'):\n",
    "                # happier -> happy er\n",
    "                if word[-3:] == 'ier' and word[:-3] + 'y' in self.vocab:\n",
    "                    lemmatizable_words[word] =  lemmatize(word, '_ier->_y+er')\n",
    "                # canner -> can er\n",
    "                elif word[-4] == word[-3] and word[:-3] in self.vocab:\n",
    "                    lemmatizable_words[word] = lemmatize(word, '_?er->_+er')\n",
    "                # baker -> bake er\n",
    "                elif word[:-1] in self.vocab:\n",
    "                    lemmatizable_words[word] = lemmatize(word, '_r->_+er')\n",
    "                # mixer -> mix er\n",
    "                elif word[:-2] in self.vocab:\n",
    "                    lemmatizable_words[word] = lemmatize(word, '_er->_+er')\n",
    "\n",
    "            # ord ends in 'ed'\n",
    "            elif (word != 'ed') and word[-2:] == 'ed':\n",
    "                # carried -> carry ed\n",
    "                if word[-3:] == 'ied' and word[:-3] + 'y' in self.vocab:\n",
    "                    lemmatizable_words[word] =  lemmatize(word, '_ied->_y+ed')\n",
    "                # canned -> can ed\n",
    "                elif word[-4] == word[-3] and word[:-3] in self.vocab:\n",
    "                    lemmatizable_words[word] = lemmatize(word, '_?ed->_+ed')\n",
    "                # baked -> bake ed\n",
    "                elif word[:-1] in self.vocab:\n",
    "                    lemmatizable_words[word] = lemmatize(word, '_d->_+ed')\n",
    "                # mixed -> mix ed\n",
    "                elif word[:-2] in self.vocab:\n",
    "                    lemmatizable_words[word] = lemmatize(word, '_ed->_+ed')\n",
    "        \n",
    "        return lemmatizable_words\n",
    "\n",
    "\n",
    "    def replace(self, replacements):\n",
    "        replacement_set = set(replacements.keys())\n",
    "\n",
    "        self.sentences = [self.replace_sentence(sentence, replacements, replacement_set)\n",
    "                          for sentence in self.sentences]\n",
    "\n",
    "    def replace_sentence(self, sentence, replacements, replacement_set):\n",
    "        for i in range(5):\n",
    "            output_sentence = []\n",
    "            edited = False\n",
    "            for word in sentence:\n",
    "                if word not in replacement_set:\n",
    "                    output_sentence.append(word)\n",
    "                    continue\n",
    "\n",
    "                edited = True\n",
    "                output_sentence.extend(replacements[word])\n",
    "\n",
    "            if not edited:\n",
    "                break\n",
    "\n",
    "            sentence = output_sentence\n",
    "        return output_sentence\n",
    "\n",
    "                 \n",
    "    # CRVs\n",
    "\n",
    "    def create_vectorizer(self, window_size = 2, removal_threshold = 0, log = True):\n",
    "        signatures = self.create_signatures(window_size, log = False)\n",
    "\n",
    "        if removal_threshold > 0:\n",
    "            for sig_word in [word for word in self.set_vocab if self.word_counts[word] <= removal_threshold]:\n",
    "                for word in self.vocab:\n",
    "                    popped = signatures[word].pop(sig_word)\n",
    "                    if popped is not None:\n",
    "                        signatures[word]['<UNK>'] += popped\n",
    "\n",
    "        encoding_vocab = list(sort_hl({word:self.word_counts[word] for word in self.set_vocab if self.word_counts[word] > removal_threshold}).keys())\n",
    "        encoding_indices = {word:i for i, word in enumerate(encoding_vocab)}\n",
    "\n",
    "        try:\n",
    "            matrix = np.zeros((len(self.vocab), len(encoding_vocab)))# how to do\n",
    "        except:# could break because vocab doesnt exist yet, and will give wrong message\n",
    "            raise Exception(\"Vocab is too big. Consider using create_signatures instead\")\n",
    "\n",
    "        for word in self.vocab:\n",
    "            for sig_word in signatures[word]:\n",
    "                matrix[self.word_indices[word], encoding_indices[sig_word]] = signatures[word][sig_word]\n",
    "        \n",
    "        if log:\n",
    "            print(\"Vectorizer created:\")\n",
    "            print(f\"    - {len(self.vocab)} unique words;\")\n",
    "            print(f\"    - {len(encoding_vocab)} CRV words\")\n",
    "\n",
    "        return Vectorizer(self.vocab, matrix, encoding_vocab)\n",
    "        \n",
    "        \n",
    "    # a Vectorizer requires a matrix. If it's too large, this makes CRVs instead\n",
    "    def create_signatures(self, window_size = 2, log = True):\n",
    "\n",
    "        # setup signatures\n",
    "        \n",
    "        signatures = { word : defaultdict(int) for word in self.vocab }\n",
    "        \n",
    "        # collect word counts for each word\n",
    "\n",
    "        for sentence in self.sentences:\n",
    "            \n",
    "            for window_center in range(len(sentence)):\n",
    "\n",
    "                # the word that we will collect signatures for\n",
    "                # around it\n",
    "                center_word = sentence[window_center]\n",
    "\n",
    "                for word_index in range(max(0, window_center - window_size), min(window_center + window_size + 1, len(sentence))):\n",
    "\n",
    "                    # while sliding the window, don't include the given word, which will always be in the center of the window\n",
    "                    if word_index == window_center:\n",
    "                        continue\n",
    "\n",
    "                    # add one to the count for that word's occurence next to the given word\n",
    "                    nearby_word = sentence[word_index]\n",
    "                    signatures[center_word][nearby_word] += 1\n",
    "\n",
    "\n",
    "        # divide signatures by counts\n",
    "        for word in self.vocab:\n",
    "            total_co_occurences = sum(signatures[word].values())\n",
    "            signatures[word] = CRV({key: val / total_co_occurences for key, val in signatures[word].items()})\n",
    "\n",
    "        # log data\n",
    "        if not log:\n",
    "            return signatures\n",
    "        \n",
    "        print(\"Signatures collected:\")\n",
    "        sorted_best = [[word, list(signatures[word].items())[0]] for word in self.vocab]\n",
    "        highest = max(sorted_best, key = lambda item : item[1][1])\n",
    "        print(\"    - highest best signature: \" + str(highest) + \" & \" + str(sorted_best.count(highest) - 1) + \" others.\")\n",
    "        lowest = min(sorted_best, key = lambda item : item[1][1])\n",
    "        print(\"    - lowest best signature: \" + str(lowest) + \" & \" + str(sorted_best.count(lowest) - 1) + \" others.\")\n",
    "\n",
    "        return signatures\n",
    "\n",
    "\n",
    "    # Corpus Search\n",
    "    \n",
    "    def find(self, words, max_seperation = 3, max_prints = 10, print_size = 20):\n",
    "        # once you find a set, skip past the first findable word, or you get duplicates\n",
    "        prints_so_far = 0\n",
    "        total_found = 0\n",
    "\n",
    "        if type(words) == str:\n",
    "            words = [words]\n",
    "\n",
    "        common_sentences = set(self.sentence_indices[words[0]]).union(*[self.sentence_indices[word] for word in words])\n",
    "        valid_sentences = set()\n",
    "\n",
    "        if len(common_sentences) == 0:\n",
    "            print('None Found')\n",
    "            return common_sentences\n",
    "        \n",
    "        for sentence_index in common_sentences:\n",
    "            sentence = self.sentences[sentence_index]\n",
    "\n",
    "            last_index = -100\n",
    "\n",
    "            for i in range(len(sentence)):\n",
    "\n",
    "                sentence_slice = sentence[i:min(len(sentence), i + max_seperation)]\n",
    "\n",
    "                slice_valid = True\n",
    "                for search_word in words:\n",
    "                    if search_word not in sentence_slice:\n",
    "                       slice_valid = False\n",
    "                       break\n",
    "                    sentence_slice.remove(search_word)\n",
    "\n",
    "                if (i - last_index) < max_seperation:\n",
    "                    slice_valid = False\n",
    "\n",
    "                if not slice_valid:\n",
    "                    continue\n",
    "\n",
    "                if prints_so_far < max_prints:\n",
    "                    print_center = i + len(sentence_slice) // 2\n",
    "                    print(concat_sentence(sentence[max(0, print_center - print_size // 2) : min(len(sentence), print_center + print_size // 2)], self.text_mode))\n",
    "                    print('\\n \\n')\n",
    "                    prints_so_far += 1\n",
    "                    last_index = i\n",
    "\n",
    "                valid_sentences.add(sentence_index)\n",
    "                total_found += 1\n",
    "\n",
    "        print(f\"Total Found : {total_found}\")\n",
    "        print(f\"Found in {len(valid_sentences)} sentences\")\n",
    "\n",
    "        return valid_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ffb8841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collected:\n",
      "    - 4107 unique words found.\n",
      "    - most common words: ['\\n', '-', '.', '1', ',']\n",
      "    - least common words: ['www', 'yet', 'zinfandel', 'zip', 'zipper']\n",
      "\n",
      "\n",
      "Corpus loaded:\n",
      "    - 5000 sentences.\n",
      "    - longest sentence: \n",
      "\n",
      "<START> whole wheat yeast bread \n",
      " \n",
      " ingredients : \n",
      " - 1 cup ( 2 2 5 ml ) scalded milk \n",
      " - 1 / 4 cup ( 6 0 ml ) lard \n",
      " - 4 tsp ( 2 0 ml ) . salt \n",
      " - 1 / 4 cup ( 6 0 ml ) honey \n",
      " - 1 cup ( 2 2 5 ml ) water , 1 1 0 - 1 1 5 degrees \n",
      " - 2 pkg . dry yeast \n",
      " - 2 tbsp ( 3 0 ml ) . brown sugar \n",
      " - 2 cups ( 4 7 5 ml ) whole wheat flour \n",
      " - 3 cups ( 7 0 0 ml ) or more white flour \n",
      " \n",
      " directions : \n",
      " - mix scalded milk , lard , salt & honey and let cool to lukewarm - about 8 5 degrees ( 3 0 c . ) . \n",
      " - mix water , yeast & brown sugar and let stand to dissolve for 2 - 3 minutes . \n",
      " - add yeast mixture to milk mixture . \n",
      " - add whole wheat flour and enough white flour to make dough . \n",
      " - mix well . \n",
      " - knead 5 - 1 0 minutes . \n",
      " - put in bowl to rise until double in size . \n",
      " - punch down and divide into two portions . \n",
      " - let rest 5 - 1 0 minutes and then shape into two loaves . \n",
      " - let rise in pans one hour or until double in bulk . \n",
      " - bake at 3 7 5 degrees ( 2 0 0 c . ) for 5 minutes and 3 5 0 degrees ( 1 7 5 c . ) for 3 0 - 3 5 minutes . \n",
      " - remove from pans and let cool thoroughly before wrapping <END>\n",
      ".    \n",
      "    - shortest sentence: \n",
      "\n",
      "<START> test \n",
      " \n",
      " ingredients : \n",
      " - 1 kg pork \n",
      " \n",
      " directions : \n",
      " - combine the first 3 ingredients <END>\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    corpus = Corpus(\n",
    "                filepath = '../corpora/recipes',\n",
    "                spell_correct = '../corpora/spelling_dictionary',\n",
    "                lemmatize = False,\n",
    "                removal_threshold = 1) # remove any words that appear only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76e80881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " ingredients : \n",
      " - 1 / 3 cup kraft grated parmesan cheese \n",
      " - 1 / 4 cup\n",
      "\n",
      " \n",
      "\n",
      "( nestle cream ) \n",
      " - 1 chop pieces of kraft cheese \n",
      " \n",
      " directions : \n",
      " - combine all\n",
      "\n",
      " \n",
      "\n",
      "2 / 0 9 \n",
      " - 1 / 3 cup kraft crunchy peanut butter \n",
      " - 2 0 0 ml\n",
      "\n",
      " \n",
      "\n",
      "1 0 <UNK> sausages \n",
      " - 1 / 2 cup kraft smooth peanut butter \n",
      " - 2 teaspoons oil \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "quartered yukon gold potatoes \n",
      " - 3 / 4 cup kraft real mayo mayonnaise \n",
      " - 1 tbsp . grey\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " - 2 avocados \n",
      " - 1 / 4 cup kraft spicy ranch dressing \n",
      " - 2 cloves garlic ,\n",
      "\n",
      " \n",
      "\n",
      "refrigerated crescent dinner rolls \n",
      " - 1 / 4 cup kraft shredded cheddar cheese \n",
      " \n",
      " directions : \n",
      " -\n",
      "\n",
      " \n",
      "\n",
      "9 dry 0 2 / 0 9 \n",
      " - 1 kraft singles \n",
      " \n",
      " directions : \n",
      " - mix dressing\n",
      "\n",
      " \n",
      "\n",
      "thawed \n",
      " - 1 - 1 / 2 cups pure kraft refrigerated barbecue ranch dressing \n",
      " - 3 cups kraft\n",
      "\n",
      " \n",
      "\n",
      "pure kraft refrigerated barbecue ranch dressing \n",
      " - 3 cups kraft shredded medium cheddar cheese \n",
      " - 1 - 1\n",
      "\n",
      " \n",
      "\n",
      "Total Found : 96\n",
      "Found in 77 sentences\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    corpus.find('kraft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "23efbe34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blueberry\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    print(random.choice(corpus.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9260ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> bread pudding \n",
      " \n",
      " ingredients : \n",
      " - 2 c . sugar \n",
      " - 1 c . butter ( 2 sticks ) \n",
      " - 6 eggs , beaten \n",
      " - 2 1 / 2 c . milk \n",
      " - 2 tbsp . nutmeg \n",
      " - 2 tbsp . vanilla \n",
      " - 1 c . raisins \n",
      " - 1 loaf of sliced bread , cubed \n",
      " \n",
      " directions : \n",
      " - preheat oven to 3 5 0 Â° . \n",
      " - cream sugar and butter together . \n",
      " - add beaten eggs , milk , nutmeg , vanilla , raisins and the cubed bread . stir to mix . \n",
      " - pour into a greased casserole dish and bake for 1 hour . \n",
      " - stir , mixture and continue to cook for 1 hour more . <END>\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    print(concat_sentence(random.choice(corpus.sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6defda3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

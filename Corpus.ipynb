{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f888947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Utils.ipynb\n",
    "%run CRV.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc2f449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from spellwise import Levenshtein, CaverphoneOne, CaverphoneTwo, Editex, Soundex, Typox\n",
    "from itertools import *\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e40942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################ The Corpus Class ################\n",
    "#\n",
    "# -Stores a list of sentences and information about them\n",
    "# -Loads and cleans data\n",
    "# -Optionally lemmatizes (cleanliness -> clean ly ness) and corrects spelling errors\n",
    "# -Splits each sentence into a list of words\n",
    "# -Collects information\n",
    "# -Creates CRVs\n",
    "# -Creates a vectorizer\n",
    "\n",
    "\n",
    "class Corpus:\n",
    "    def __init__(self,\n",
    "                 filepath,                # no need for '.json'\n",
    "                 file_indexer = None,     # some jsons have metadata, and so you need to index the actual text\n",
    "                 text_mode = 'word',      # 'word' or 'char'\n",
    "                 spell_correct = False,   # spell correct (fails on niche words)\n",
    "                 lemmatize = True,        # remove 'er', 'ed', 's', 'ing', etc. to the best of its ability\n",
    "                 removal_threshold = 0,   # replace a word with <= this many appearences in the corpus with <UNK>\n",
    "                 log = True):\n",
    "\n",
    "        # setup\n",
    "        self.text_mode = text_mode\n",
    "        self.load_sentences(filepath, file_indexer)\n",
    "\n",
    "        # clean data\n",
    "        if spell_correct != False:\n",
    "            self.spell_correcter = CaverphoneOne()\n",
    "            self.spell_correcter.add_from_path(spell_correct)\n",
    "            self.spell_correct()\n",
    "\n",
    "        self.lemmatize() if lemmatize else None\n",
    "        self.remove_uncommon(removal_threshold) if removal_threshold >= 1 else None\n",
    "        \n",
    "        # post_cleanup regathering\n",
    "        self.scrape_data()\n",
    "\n",
    "        # log info\n",
    "        if not log:\n",
    "            return\n",
    "\n",
    "        print(\"Corpus loaded:\")\n",
    "        print(\"    - \" + str(len(self.sentences)) + \" sentences.\")\n",
    "        print(\"    - longest sentence: \\n\")\n",
    "        print(concat_sentence(max(self.sentences, key = len)))\n",
    "        print(\".    \")\n",
    "        print(\"    - shortest sentence: \\n\")\n",
    "        print(concat_sentence(min(self.sentences, key = len)))\n",
    "\n",
    "\n",
    "    # Data loading and normalization\n",
    "\n",
    "    def load_sentences(self, filepath, file_indexer = None):\n",
    "        if filepath[-5:] != '.json':\n",
    "            filepath += '.json'\n",
    "\n",
    "        with open(filepath) as f:\n",
    "            if file_indexer is None:\n",
    "                self.sentences = json.load(f)\n",
    "            else:\n",
    "                self.sentences = [item[file_indexer] for item in json.load(f)]\n",
    "\n",
    "        self.sentences = [sentence.lower() for sentence in self.sentences]\n",
    "        self.sentences = list(set(self.sentences))\n",
    "        self.sentences = [split_sentence(sentence, self.text_mode) for sentence in self.sentences]\n",
    "\n",
    "    # data collection\n",
    "\n",
    "    def get_word_counts_and_vocab(self):\n",
    "        self.word_counts = sort_hl(dict(Counter(chain(*self.sentences))))\n",
    "        self.vocab = list(self.word_counts.keys())\n",
    "        self.set_vocab = set(self.vocab)\n",
    "        \n",
    "\n",
    "    def scrape_data(self, log = True):\n",
    "\n",
    "        self.get_word_counts_and_vocab()\n",
    "\n",
    "        self.sentence_indices = defaultdict(set)\n",
    "\n",
    "        for i, sentence in enumerate(self.sentences):\n",
    "            for word in set(sentence):\n",
    "                self.sentence_indices[word].add(i)\n",
    "\n",
    "        self.word_indices = {word : i for i, word in enumerate(self.vocab)}\n",
    "        self.total_word_count = sum(self.word_counts.values())\n",
    "        self.total_unique_word_count = len(self.vocab)\n",
    "        self.word_percentages = {item[0] : item[1] / self.total_word_count for item in self.word_counts.items()}\n",
    "        self.max_length = len(max(self.sentences, key = len))\n",
    "\n",
    "        # log info\n",
    "        if not log:\n",
    "            return\n",
    "    \n",
    "        print(\"Data collected:\")\n",
    "        print(\"    - \" + str(self.total_unique_word_count) + \" unique words found.\")\n",
    "        print(\"    - most common words: \" + str(list(self.word_counts.keys())[:5]))\n",
    "        print(\"    - least common words: \" + str(list(self.word_counts.keys())[-5:]))\n",
    "\n",
    "\n",
    "    # Data cleanup\n",
    "\n",
    "    def spell_correct(self):\n",
    "        self.get_word_counts_and_vocab()\n",
    "        self.replace(self.get_correctable_words())\n",
    "\n",
    "    def lemmatize(self):\n",
    "        self.get_word_counts_and_vocab()\n",
    "        self.replace(self.get_lemmatizable_words())\n",
    "\n",
    "    def remove_uncommon(self, n = 0):\n",
    "        self.get_word_counts_and_vocab()\n",
    "        self.replace({word : ['<UNK>'] for word in self.set_vocab if self.word_counts[word] <= n})        \n",
    "\n",
    "    def get_correctable_words(self, word_set = None):\n",
    "        possible_corrections = defaultdict(str)\n",
    "\n",
    "        for word in word_set or (self.set_vocab - set(['<START>', '<END>', '<UNK>'])):\n",
    "            result = self.get_correction(word)\n",
    "            if result != word:\n",
    "                possible_corrections[word] = result\n",
    "\n",
    "        return possible_corrections\n",
    "    \n",
    "    def get_correction(self, word):\n",
    "        if len(word) <= 3:\n",
    "            return word\n",
    "\n",
    "        corrections = self.spell_correcter.get_suggestions(word)\n",
    "        corrections = {item['word'] : item['distance'] for item in corrections}\n",
    "        same_keys = set(corrections.keys()).intersection(self.set_vocab)\n",
    "\n",
    "        if len(same_keys) == 0:\n",
    "            return word\n",
    "\n",
    "        if word in same_keys:\n",
    "            return word\n",
    "\n",
    "        corrections = {key : self.word_counts[key] for key in same_keys}\n",
    "        return list(sort_hl(corrections).keys())[0:1]\n",
    "    \n",
    "\n",
    "    def spelled_correct(self, word):\n",
    "        return self.get_correction(word) == word\n",
    "\n",
    "\n",
    "\n",
    "    def get_lemmatizable_words(self, word_set = None):\n",
    "        lemmatizable_words = defaultdict(str)\n",
    "\n",
    "        for word in word_set or self.vocab:\n",
    "            if len(word) <= 4:\n",
    "                continue\n",
    "\n",
    "            # word ends in 's'\n",
    "            if (word[-1] == 's') and (word[-2:] != 'ss') and (word[-3:] != 'ous'):\n",
    "                # some words with a y change spelling (candy -> candies)\n",
    "                if word[-3:] == 'ies' and word[:-3] + 'y' in self.vocab:\n",
    "                    lemmatizable_words[word] = lemmatize(word, '_ies->_y+s')\n",
    "                # words that normally end in 's' have 'es' added\n",
    "                elif word[-3:] == 'ses' and word[:-2] in self.vocab:      \n",
    "                    lemmatizable_words[word] = lemmatize(word, '_es->_+s')\n",
    "                # words uses usual 's' rules\n",
    "                elif word[:-1] in self.vocab:\n",
    "                    lemmatizable_words[word] = lemmatize(word, '_s->_+s')\n",
    "\n",
    "            # word ends in 'ly'\n",
    "            elif word[-2:] == 'ly':\n",
    "                if word[:-1] + 'e' in self.set_vocab:\n",
    "                    lemmatizable_words[word] = lemmatize(word, '_y->_e+ly')# fully\n",
    "                elif word[:-2] in self.set_vocab:\n",
    "                    lemmatizable_words[word] = lemmatize(word, '_ly->_+ly')# fully\n",
    "\n",
    "            # word ends in 'ness'\n",
    "            elif word[-4:] == 'ness':\n",
    "                lemmatizable_words[word] = lemmatize(word, '_ness->_+ness')\n",
    "\n",
    "            elif word[-4:] == 'less' and word != 'less':\n",
    "                lemmatizable_words[word] = lemmatize(word, '_less->_+less')\n",
    "\n",
    "            # word ends in 'ing'\n",
    "            elif word [-3:] == 'ing':\n",
    "                # putting -> put ing\n",
    "                if word[-5] == word[-4] and word[:-4] in self.vocab:\n",
    "                    lemmatizable_words[word] = lemmatize(word, '_?ing->_+ing')\n",
    "                # baking -> bake ing\n",
    "                elif word[:-3] + 'e' in self.vocab:\n",
    "                    lemmatizable_words[word] = lemmatize(word, '_ing->_e+ing')\n",
    "                # working -> work ing\n",
    "                elif word[:-3] in self.vocab and len(word) > 5:\n",
    "                    lemmatizable_words[word] = lemmatize(word, '_ing->_+ing')\n",
    "\n",
    "            # word ends in 'er'\n",
    "            elif (word != 'er') and (word[-2:] == 'er'):\n",
    "                # happier -> happy er\n",
    "                if word[-3:] == 'ier' and word[:-3] + 'y' in self.vocab:\n",
    "                    lemmatizable_words[word] =  lemmatize(word, '_ier->_y+er')\n",
    "                # canner -> can er\n",
    "                elif word[-4] == word[-3] and word[:-3] in self.vocab:\n",
    "                    lemmatizable_words[word] = lemmatize(word, '_?er->_+er')\n",
    "                # baker -> bake er\n",
    "                elif word[:-1] in self.vocab:\n",
    "                    lemmatizable_words[word] = lemmatize(word, '_r->_+er')\n",
    "                # mixer -> mix er\n",
    "                elif word[:-2] in self.vocab:\n",
    "                    lemmatizable_words[word] = lemmatize(word, '_er->_+er')\n",
    "\n",
    "            # ord ends in 'ed'\n",
    "            elif (word != 'ed') and word[-2:] == 'ed':\n",
    "                # carried -> carry ed\n",
    "                if word[-3:] == 'ied' and word[:-3] + 'y' in self.vocab:\n",
    "                    lemmatizable_words[word] =  lemmatize(word, '_ied->_y+ed')\n",
    "                # canned -> can ed\n",
    "                elif word[-4] == word[-3] and word[:-3] in self.vocab:\n",
    "                    lemmatizable_words[word] = lemmatize(word, '_?ed->_+ed')\n",
    "                # baked -> bake ed\n",
    "                elif word[:-1] in self.vocab:\n",
    "                    lemmatizable_words[word] = lemmatize(word, '_d->_+ed')\n",
    "                # mixed -> mix ed\n",
    "                elif word[:-2] in self.vocab:\n",
    "                    lemmatizable_words[word] = lemmatize(word, '_ed->_+ed')\n",
    "        \n",
    "        return lemmatizable_words\n",
    "\n",
    "\n",
    "    def replace(self, replacements):\n",
    "        replacement_set = set(replacements.keys())\n",
    "\n",
    "        self.sentences = [self.replace_sentence(sentence, replacements, replacement_set)\n",
    "                          for sentence in self.sentences]\n",
    "\n",
    "    def replace_sentence(self, sentence, replacements, replacement_set):\n",
    "        for i in range(5):\n",
    "            output_sentence = []\n",
    "            edited = False\n",
    "            for word in sentence:\n",
    "                if word not in replacement_set:\n",
    "                    output_sentence.append(word)\n",
    "                    continue\n",
    "\n",
    "                edited = True\n",
    "                output_sentence.extend(replacements[word])\n",
    "\n",
    "            if not edited:\n",
    "                break\n",
    "\n",
    "            sentence = output_sentence\n",
    "        return output_sentence\n",
    "\n",
    "                 \n",
    "    # CRVs\n",
    "\n",
    "    def create_vectorizer(self, window_size = 2, removal_threshold = 0, log = True):\n",
    "        signatures = self.create_signatures(window_size, log = False)\n",
    "\n",
    "        if removal_threshold > 0:\n",
    "            for sig_word in [word for word in self.set_vocab if self.word_counts[word] <= removal_threshold]:\n",
    "                for word in self.vocab:\n",
    "                    popped = signatures[word].pop(sig_word)\n",
    "                    if popped is not None:\n",
    "                        signatures[word]['<UNK>'] += popped\n",
    "\n",
    "        encoding_vocab = list(sort_hl({word:self.word_counts[word] for word in self.set_vocab if self.word_counts[word] > removal_threshold}).keys())\n",
    "        encoding_indices = {word:i for i, word in enumerate(encoding_vocab)}\n",
    "\n",
    "        try:\n",
    "            matrix = np.zeros((len(self.vocab), len(encoding_vocab)))# how to do\n",
    "        except:# could break because vocab doesnt exist yet, and will give wrong message\n",
    "            raise Exception(\"Vocab is too big. Consider using create_signatures instead\")\n",
    "\n",
    "        for word in self.vocab:\n",
    "            for sig_word in signatures[word]:\n",
    "                matrix[self.word_indices[word], encoding_indices[sig_word]] = signatures[word][sig_word]\n",
    "        \n",
    "        if log:\n",
    "            print(\"Vectorizer created:\")\n",
    "            print(f\"    - {len(self.vocab)} unique words;\")\n",
    "            print(f\"    - {len(encoding_vocab)} CRV words\")\n",
    "\n",
    "        return Vectorizer(self.vocab, matrix, encoding_vocab)\n",
    "        \n",
    "        \n",
    "    # a Vectorizer requires a matrix. If it's too large, this makes CRVs instead\n",
    "    def create_signatures(self, window_size = 2, log = True):\n",
    "\n",
    "        # setup signatures\n",
    "        \n",
    "        signatures = { word : defaultdict(int) for word in self.vocab }\n",
    "        \n",
    "        # collect word counts for each word\n",
    "\n",
    "        for sentence in self.sentences:\n",
    "            \n",
    "            for window_center in range(len(sentence)):\n",
    "\n",
    "                # the word that we will collect signatures for\n",
    "                # around it\n",
    "                center_word = sentence[window_center]\n",
    "\n",
    "                for word_index in range(max(0, window_center - window_size), min(window_center + window_size + 1, len(sentence))):\n",
    "\n",
    "                    # while sliding the window, don't include the given word, which will always be in the center of the window\n",
    "                    if word_index == window_center:\n",
    "                        continue\n",
    "\n",
    "                    # add one to the count for that word's occurence next to the given word\n",
    "                    nearby_word = sentence[word_index]\n",
    "                    signatures[center_word][nearby_word] += 1\n",
    "\n",
    "\n",
    "        # divide signatures by counts\n",
    "        for word in self.vocab:\n",
    "            total_co_occurences = sum(signatures[word].values())\n",
    "            signatures[word] = CRV({key: val / total_co_occurences for key, val in signatures[word].items()})\n",
    "\n",
    "        # log data\n",
    "        if not log:\n",
    "            return signatures\n",
    "        \n",
    "        print(\"Signatures collected:\")\n",
    "        sorted_best = [[word, list(signatures[word].items())[0]] for word in self.vocab]\n",
    "        highest = max(sorted_best, key = lambda item : item[1][1])\n",
    "        print(\"    - highest best signature: \" + str(highest) + \" & \" + str(sorted_best.count(highest) - 1) + \" others.\")\n",
    "        lowest = min(sorted_best, key = lambda item : item[1][1])\n",
    "        print(\"    - lowest best signature: \" + str(lowest) + \" & \" + str(sorted_best.count(lowest) - 1) + \" others.\")\n",
    "\n",
    "        return signatures\n",
    "\n",
    "\n",
    "    # Corpus Search\n",
    "    \n",
    "    def find(self, words, max_seperation = 1, max_prints = 10, print_size = 20):\n",
    "        # once you find a set, skip past the first findable word, or you get duplicates\n",
    "        prints_so_far = 0\n",
    "        total_found = 0\n",
    "\n",
    "        if type(words) == str:\n",
    "            words = [words]\n",
    "\n",
    "        common_sentences = set(self.sentence_indices[words[0]]).union(*[self.sentence_indices[word] for word in words])\n",
    "        valid_sentences = set()\n",
    "\n",
    "        if len(common_sentences) == 0:\n",
    "            print('None Found')\n",
    "            return common_sentences\n",
    "        \n",
    "        for sentence_index in common_sentences:\n",
    "            sentence = self.sentences[sentence_index]\n",
    "\n",
    "            for i in range(len(sentence)):\n",
    "\n",
    "                sentence_slice = sentence[i:min(len(sentence), i + max_seperation + 1)]\n",
    "\n",
    "                slice_valid = True\n",
    "                for search_word in words:\n",
    "                    if search_word not in sentence_slice:\n",
    "                       slice_valid = False\n",
    "                       break\n",
    "                    sentence_slice.remove(search_word)\n",
    "\n",
    "                if not slice_valid:\n",
    "                    continue\n",
    "\n",
    "                if prints_so_far < max_prints:\n",
    "                    print_center = i + len(sentence_slice) // 2\n",
    "                    print(concat_sentence(sentence[max(0, print_center - print_size // 2) : min(len(sentence), print_center + print_size // 2)], self.text_mode))\n",
    "                    print('\\n \\n')\n",
    "                    prints_so_far += 1\n",
    "\n",
    "                valid_sentences.add(sentence_index)\n",
    "                total_found += 1\n",
    "\n",
    "        print(f\"Total Found : {total_found}\")\n",
    "        print(f\"Found in {len(valid_sentences)} sentences\")\n",
    "\n",
    "        return valid_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ffb8841",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'corpora/recipes.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     corpus = \u001b[43mCorpus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcorpora/recipes\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m                \u001b[49m\u001b[43mspell_correct\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mspelling_dictionary\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m                \u001b[49m\u001b[43mlemmatize\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m                \u001b[49m\u001b[43mremoval_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# remove any words that appear only once\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mCorpus.__init__\u001b[39m\u001b[34m(self, filepath, file_indexer, text_mode, spell_correct, lemmatize, removal_threshold, log)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[32m     14\u001b[39m              filepath,                \u001b[38;5;66;03m# no need for '.json'\u001b[39;00m\n\u001b[32m     15\u001b[39m              file_indexer = \u001b[38;5;28;01mNone\u001b[39;00m,     \u001b[38;5;66;03m# some jsons have metadata, and so you need to index the actual text\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m \n\u001b[32m     22\u001b[39m     \u001b[38;5;66;03m# setup\u001b[39;00m\n\u001b[32m     23\u001b[39m     \u001b[38;5;28mself\u001b[39m.text_mode = text_mode\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_indexer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     \u001b[38;5;66;03m# clean data\u001b[39;00m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m spell_correct != \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mCorpus.load_sentences\u001b[39m\u001b[34m(self, filepath, file_indexer)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filepath[-\u001b[32m5\u001b[39m:] != \u001b[33m'\u001b[39m\u001b[33m.json\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     55\u001b[39m     filepath += \u001b[33m'\u001b[39m\u001b[33m.json\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m file_indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     59\u001b[39m         \u001b[38;5;28mself\u001b[39m.sentences = json.load(f)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/CRV-Playground/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'corpora/recipes.json'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    corpus = Corpus(\n",
    "                filepath = 'corpora/recipes',\n",
    "                spell_correct = 'spelling_dictionary',\n",
    "                lemmatize = False,\n",
    "                removal_threshold = 1) # remove any words that appear only once"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
